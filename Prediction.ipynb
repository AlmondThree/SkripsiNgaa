{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "from gensim import models\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import collections\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('raw_data.csv')\n",
    "data['Sentimen'] = pd.to_numeric(data['Sentimen'], errors='coerce').fillna(0)\n",
    "data['Sentimen'] = data.Sentimen.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nama</th>\n",
       "      <th>ulasan_</th>\n",
       "      <th>tanggal</th>\n",
       "      <th>Sentimen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ransi Lintin</td>\n",
       "      <td>Aplikasinya bagus</td>\n",
       "      <td>2022-02-20 00:07:51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HAHAHAHA HAHAHA</td>\n",
       "      <td>Ok mantap</td>\n",
       "      <td>2022-02-20 00:48:59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yundi Hartono</td>\n",
       "      <td>Mantap semoga semakin maju</td>\n",
       "      <td>2022-02-20 01:33:10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nunu Nugraha</td>\n",
       "      <td>Bisa hemat biaya tf mantap ini sih rekomen ban...</td>\n",
       "      <td>2022-02-20 04:15:04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bahtiar hamzah</td>\n",
       "      <td>Mantap</td>\n",
       "      <td>2022-02-20 04:22:43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              nama                                            ulasan_  \\\n",
       "0     Ransi Lintin                                  Aplikasinya bagus   \n",
       "1  HAHAHAHA HAHAHA                                          Ok mantap   \n",
       "2    Yundi Hartono                         Mantap semoga semakin maju   \n",
       "3     Nunu Nugraha  Bisa hemat biaya tf mantap ini sih rekomen ban...   \n",
       "4   bahtiar hamzah                                             Mantap   \n",
       "\n",
       "               tanggal  Sentimen  \n",
       "0  2022-02-20 00:07:51         1  \n",
       "1  2022-02-20 00:48:59         1  \n",
       "2  2022-02-20 01:33:10         1  \n",
       "3  2022-02-20 04:15:04         1  \n",
       "4  2022-02-20 04:22:43         1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns = ['nama', 'ulasan_', 'tanggal', 'Sentimen']\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Sentimen.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2091, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = []\n",
    "neg = []\n",
    "for l in data.Sentimen:\n",
    "    if l == 0:\n",
    "        pos.append(0)\n",
    "        neg.append(1)\n",
    "    elif l == 1:\n",
    "        pos.append(int(1))\n",
    "        neg.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nama</th>\n",
       "      <th>ulasan_</th>\n",
       "      <th>tanggal</th>\n",
       "      <th>Sentimen</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ransi Lintin</td>\n",
       "      <td>Aplikasinya bagus</td>\n",
       "      <td>2022-02-20 00:07:51</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HAHAHAHA HAHAHA</td>\n",
       "      <td>Ok mantap</td>\n",
       "      <td>2022-02-20 00:48:59</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yundi Hartono</td>\n",
       "      <td>Mantap semoga semakin maju</td>\n",
       "      <td>2022-02-20 01:33:10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nunu Nugraha</td>\n",
       "      <td>Bisa hemat biaya tf mantap ini sih rekomen ban...</td>\n",
       "      <td>2022-02-20 04:15:04</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bahtiar hamzah</td>\n",
       "      <td>Mantap</td>\n",
       "      <td>2022-02-20 04:22:43</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>muaddi aja</td>\n",
       "      <td>Selama ini bagus  transaksi lancar</td>\n",
       "      <td>2022-02-20 05:04:37</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>JANUARDI .</td>\n",
       "      <td>Transaksi cepat tanpa biaya antar bank</td>\n",
       "      <td>2022-02-20 05:05:24</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mudi Pangestu</td>\n",
       "      <td>Aplikasi ya bagus sangat membantu Makasi flip ...</td>\n",
       "      <td>2022-02-20 05:06:48</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TRISNANDA HILMI</td>\n",
       "      <td>Alhamdulillah. Banyak ngebantu dg aplikasi ini</td>\n",
       "      <td>2022-02-20 05:23:13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Padry Alfath</td>\n",
       "      <td>aplikasi terbaik yang sangat membantu pekerjaa...</td>\n",
       "      <td>2022-02-20 05:43:46</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Aris Munandar</td>\n",
       "      <td>Sangat membantu</td>\n",
       "      <td>2022-02-20 06:17:13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Mecca 1B</td>\n",
       "      <td>Sangat membantu transfer antar bank</td>\n",
       "      <td>2022-02-20 06:27:16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Luki Suryana</td>\n",
       "      <td>Flip ok banget guys...</td>\n",
       "      <td>2022-02-20 06:40:25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Ihwan A. Gani</td>\n",
       "      <td>bagus bangetz...</td>\n",
       "      <td>2022-02-20 06:47:38</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Suwarti Karmana</td>\n",
       "      <td>Transfer flip  tidak buat kantong bolong</td>\n",
       "      <td>2022-02-20 06:54:25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>sunnatun nahriyati</td>\n",
       "      <td>Keren  menghemat biaya transfer..</td>\n",
       "      <td>2022-02-20 07:01:41</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ainun jariyah</td>\n",
       "      <td>Kak  saya sudah top up 300 eh dan mau bayar to...</td>\n",
       "      <td>2022-02-20 07:17:36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Kusni Mulyani</td>\n",
       "      <td>Praktis... dan gratis... Sangat membantu..</td>\n",
       "      <td>2022-02-20 07:19:23</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>riffan abvaliandro</td>\n",
       "      <td>Mudah dan simple  sangat membantu</td>\n",
       "      <td>2022-02-20 07:32:50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Siti Julaeha</td>\n",
       "      <td>Simple  praktis &amp; sangat membantu.. Beli pulsa...</td>\n",
       "      <td>2022-02-20 07:48:28</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  nama                                            ulasan_  \\\n",
       "0         Ransi Lintin                                  Aplikasinya bagus   \n",
       "1      HAHAHAHA HAHAHA                                          Ok mantap   \n",
       "2        Yundi Hartono                         Mantap semoga semakin maju   \n",
       "3         Nunu Nugraha  Bisa hemat biaya tf mantap ini sih rekomen ban...   \n",
       "4       bahtiar hamzah                                             Mantap   \n",
       "5           muaddi aja                 Selama ini bagus  transaksi lancar   \n",
       "6          JANUARDI .              Transaksi cepat tanpa biaya antar bank   \n",
       "7        Mudi Pangestu  Aplikasi ya bagus sangat membantu Makasi flip ...   \n",
       "8      TRISNANDA HILMI     Alhamdulillah. Banyak ngebantu dg aplikasi ini   \n",
       "9         Padry Alfath  aplikasi terbaik yang sangat membantu pekerjaa...   \n",
       "10       Aris Munandar                                    Sangat membantu   \n",
       "11            Mecca 1B                Sangat membantu transfer antar bank   \n",
       "12        Luki Suryana                             Flip ok banget guys...   \n",
       "13       Ihwan A. Gani                                   bagus bangetz...   \n",
       "14     Suwarti Karmana           Transfer flip  tidak buat kantong bolong   \n",
       "15  sunnatun nahriyati                  Keren  menghemat biaya transfer..   \n",
       "16       ainun jariyah  Kak  saya sudah top up 300 eh dan mau bayar to...   \n",
       "17       Kusni Mulyani         Praktis... dan gratis... Sangat membantu..   \n",
       "18  riffan abvaliandro                  Mudah dan simple  sangat membantu   \n",
       "19        Siti Julaeha  Simple  praktis & sangat membantu.. Beli pulsa...   \n",
       "\n",
       "                tanggal  Sentimen  Pos  Neg  \n",
       "0   2022-02-20 00:07:51         1    1    0  \n",
       "1   2022-02-20 00:48:59         1    1    0  \n",
       "2   2022-02-20 01:33:10         1    1    0  \n",
       "3   2022-02-20 04:15:04         1    1    0  \n",
       "4   2022-02-20 04:22:43         1    1    0  \n",
       "5   2022-02-20 05:04:37         1    1    0  \n",
       "6   2022-02-20 05:05:24         1    1    0  \n",
       "7   2022-02-20 05:06:48         1    1    0  \n",
       "8   2022-02-20 05:23:13         1    1    0  \n",
       "9   2022-02-20 05:43:46         1    1    0  \n",
       "10  2022-02-20 06:17:13         1    1    0  \n",
       "11  2022-02-20 06:27:16         1    1    0  \n",
       "12  2022-02-20 06:40:25         1    1    0  \n",
       "13  2022-02-20 06:47:38         1    1    0  \n",
       "14  2022-02-20 06:54:25         1    1    0  \n",
       "15  2022-02-20 07:01:41         1    1    0  \n",
       "16  2022-02-20 07:17:36         0    0    1  \n",
       "17  2022-02-20 07:19:23         1    1    0  \n",
       "18  2022-02-20 07:32:50         1    1    0  \n",
       "19  2022-02-20 07:48:28         1    1    0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Pos']= pd.Series(pos)\n",
    "data['Neg']= pd.Series(neg)\n",
    "\n",
    "#data['Pos']= pos\n",
    "#data['Neg']= neg\n",
    "\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nama</th>\n",
       "      <th>ulasan_</th>\n",
       "      <th>tanggal</th>\n",
       "      <th>Sentimen</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Neg</th>\n",
       "      <th>Text_Clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ransi Lintin</td>\n",
       "      <td>Aplikasinya bagus</td>\n",
       "      <td>2022-02-20 00:07:51</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Aplikasinya bagus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HAHAHAHA HAHAHA</td>\n",
       "      <td>Ok mantap</td>\n",
       "      <td>2022-02-20 00:48:59</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Ok mantap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yundi Hartono</td>\n",
       "      <td>Mantap semoga semakin maju</td>\n",
       "      <td>2022-02-20 01:33:10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Mantap semoga semakin maju</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nunu Nugraha</td>\n",
       "      <td>Bisa hemat biaya tf mantap ini sih rekomen ban...</td>\n",
       "      <td>2022-02-20 04:15:04</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Bisa hemat biaya tf mantap ini sih rekomen ban...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bahtiar hamzah</td>\n",
       "      <td>Mantap</td>\n",
       "      <td>2022-02-20 04:22:43</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Mantap</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              nama                                            ulasan_  \\\n",
       "0     Ransi Lintin                                  Aplikasinya bagus   \n",
       "1  HAHAHAHA HAHAHA                                          Ok mantap   \n",
       "2    Yundi Hartono                         Mantap semoga semakin maju   \n",
       "3     Nunu Nugraha  Bisa hemat biaya tf mantap ini sih rekomen ban...   \n",
       "4   bahtiar hamzah                                             Mantap   \n",
       "\n",
       "               tanggal  Sentimen  Pos  Neg  \\\n",
       "0  2022-02-20 00:07:51         1    1    0   \n",
       "1  2022-02-20 00:48:59         1    1    0   \n",
       "2  2022-02-20 01:33:10         1    1    0   \n",
       "3  2022-02-20 04:15:04         1    1    0   \n",
       "4  2022-02-20 04:22:43         1    1    0   \n",
       "\n",
       "                                          Text_Clean  \n",
       "0                                  Aplikasinya bagus  \n",
       "1                                          Ok mantap  \n",
       "2                         Mantap semoga semakin maju  \n",
       "3  Bisa hemat biaya tf mantap ini sih rekomen ban...  \n",
       "4                                             Mantap  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punct(ulasan_):\n",
    "    text_nopunct = ''\n",
    "    text_nopunct = re.sub('['+string.punctuation+']', '', str(ulasan_))\n",
    "    return text_nopunct\n",
    "\n",
    "data['Text_Clean'] = data['ulasan_'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, WordNetLemmatizer\n",
    "#nltk.download()\n",
    "\n",
    "tokens = [word_tokenize(sen) for sen in data.Text_Clean] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_token(tokens): \n",
    "    return [w.lower() for w in tokens]    \n",
    "    \n",
    "lower_tokens = [lower_token(token) for token in tokens] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "# stoplist = stopwords.words('english')\n",
    "\n",
    "# print(stoplist)\n",
    "\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "stoplist = StopWordRemoverFactory().get_stop_words()\n",
    "\n",
    "#print(stoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(tokens): \n",
    "    return [word for word in tokens if word not in stoplist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_words = [remove_stop_words(sen) for sen in lower_tokens] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [' '.join(sen) for sen in filtered_words] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text_Final</th>\n",
       "      <th>tokens</th>\n",
       "      <th>Sentimen</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aplikasinya bagus</td>\n",
       "      <td>[aplikasinya, bagus]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mantap</td>\n",
       "      <td>[mantap]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mantap semoga semakin maju</td>\n",
       "      <td>[mantap, semoga, semakin, maju]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hemat biaya tf mantap sih rekomen banget makas...</td>\n",
       "      <td>[hemat, biaya, tf, mantap, sih, rekomen, bange...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mantap</td>\n",
       "      <td>[mantap]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Text_Final  \\\n",
       "0                                  aplikasinya bagus   \n",
       "1                                             mantap   \n",
       "2                         mantap semoga semakin maju   \n",
       "3  hemat biaya tf mantap sih rekomen banget makas...   \n",
       "4                                             mantap   \n",
       "\n",
       "                                              tokens  Sentimen  Pos  Neg  \n",
       "0                               [aplikasinya, bagus]         1    1    0  \n",
       "1                                           [mantap]         1    1    0  \n",
       "2                    [mantap, semoga, semakin, maju]         1    1    0  \n",
       "3  [hemat, biaya, tf, mantap, sih, rekomen, bange...         1    1    0  \n",
       "4                                           [mantap]         1    1    0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Text_Final'] = result\n",
    "\n",
    "data['tokens'] = filtered_words\n",
    "\n",
    "data = data[['Text_Final', 'tokens', 'Sentimen', 'Pos', 'Neg']]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(data, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15573 words total, with a vocabulary size of 2966\n",
      "Max sentence length is 67\n"
     ]
    }
   ],
   "source": [
    "all_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1627 words total, with a vocabulary size of 663\n",
      "Max sentence length is 49\n"
     ]
    }
   ],
   "source": [
    "all_test_words = [word for tokens in data_test[\"tokens\"] for word in tokens]\n",
    "test_sentence_lengths = [len(tokens) for tokens in data_test[\"tokens\"]]\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec_path = 'idwiki_word2vec_200_new_lower.model'\n",
    "word2vec_mod = Word2Vec.load(word2vec_path)\n",
    "word2vec = word2vec_mod.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=200):\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
    "    embeddings = clean_comments['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n",
    "                                                                                generate_missing=generate_missing))\n",
    "    return list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_embeddings = get_word2vec_embeddings(word2vec, data_train, generate_missing=True)\n",
    "\n",
    "training_embeddings = get_word2vec_embeddings(word2vec, data_train, generate_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 50\n",
    "EMBEDDING_DIM = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2966 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(data_train[\"Text_Final\"].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(data_train[\"Text_Final\"].tolist())\n",
    "\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2967, 200)\n"
     ]
    }
   ],
   "source": [
    "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(data_test[\"Text_Final\"].tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False)\n",
    "    \n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4,5,6]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=200, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "\n",
    "    l_merge = concatenate(convs, axis=1)\n",
    "\n",
    "    x = Dropout(0.1)(l_merge)  \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['Pos', 'Neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data_train[label_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_cnn_data\n",
    "y_tr = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, 50, 200)      593400      ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_15 (Conv1D)             (None, 49, 200)      80200       ['embedding_3[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_16 (Conv1D)             (None, 48, 200)      120200      ['embedding_3[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_17 (Conv1D)             (None, 47, 200)      160200      ['embedding_3[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_18 (Conv1D)             (None, 46, 200)      200200      ['embedding_3[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_19 (Conv1D)             (None, 45, 200)      240200      ['embedding_3[0][0]']            \n",
      "                                                                                                  \n",
      " global_max_pooling1d_15 (Globa  (None, 200)         0           ['conv1d_15[0][0]']              \n",
      " lMaxPooling1D)                                                                                   \n",
      "                                                                                                  \n",
      " global_max_pooling1d_16 (Globa  (None, 200)         0           ['conv1d_16[0][0]']              \n",
      " lMaxPooling1D)                                                                                   \n",
      "                                                                                                  \n",
      " global_max_pooling1d_17 (Globa  (None, 200)         0           ['conv1d_17[0][0]']              \n",
      " lMaxPooling1D)                                                                                   \n",
      "                                                                                                  \n",
      " global_max_pooling1d_18 (Globa  (None, 200)         0           ['conv1d_18[0][0]']              \n",
      " lMaxPooling1D)                                                                                   \n",
      "                                                                                                  \n",
      " global_max_pooling1d_19 (Globa  (None, 200)         0           ['conv1d_19[0][0]']              \n",
      " lMaxPooling1D)                                                                                   \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 1000)         0           ['global_max_pooling1d_15[0][0]',\n",
      "                                                                  'global_max_pooling1d_16[0][0]',\n",
      "                                                                  'global_max_pooling1d_17[0][0]',\n",
      "                                                                  'global_max_pooling1d_18[0][0]',\n",
      "                                                                  'global_max_pooling1d_19[0][0]']\n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 1000)         0           ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 128)          128128      ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 128)          0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 2)            258         ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,522,786\n",
      "Trainable params: 929,386\n",
      "Non-trainable params: 593,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n",
    "                len(list(label_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "batch_size = 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "50/50 [==============================] - 12s 176ms/step - loss: 0.4645 - acc: 0.8457 - val_loss: 0.2604 - val_acc: 0.8836\n",
      "Epoch 2/3\n",
      "50/50 [==============================] - 9s 172ms/step - loss: 0.1776 - acc: 0.9468 - val_loss: 0.2266 - val_acc: 0.9048\n",
      "Epoch 3/3\n",
      "50/50 [==============================] - 9s 170ms/step - loss: 0.1010 - acc: 0.9728 - val_loss: 0.3144 - val_acc: 0.8730\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train, y_tr, epochs=num_epochs, validation_split=0.1, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 588ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_cnn_data, batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_labels=[]\n",
    "for p in predictions:\n",
    "    prediction_labels.append(labels[np.argmax(p)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9095238095238095"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data_test.Sentimen==prediction_labels)/len(prediction_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    169\n",
       "0     41\n",
       "Name: Sentimen, dtype: int64"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.Sentimen.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text_Final</th>\n",
       "      <th>tokens</th>\n",
       "      <th>Sentimen</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1939</th>\n",
       "      <td>kasih 5 bintang</td>\n",
       "      <td>[kasih, 5, bintang]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>uang ngilang ntah kmna ga recomend sama sekali...</td>\n",
       "      <td>[uang, ngilang, ntah, kmna, ga, recomend, sama...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>bagus cepat mudah dn hemat tentunya trs tingka...</td>\n",
       "      <td>[bagus, cepat, mudah, dn, hemat, tentunya, trs...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>gue kasih bintang 1 sengaja gue ikut rules yg ...</td>\n",
       "      <td>[gue, kasih, bintang, 1, sengaja, gue, ikut, r...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>cepat prosesnya terpercaya lanjutkan lebih baik</td>\n",
       "      <td>[cepat, prosesnya, terpercaya, lanjutkan, lebi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Text_Final  \\\n",
       "1939                                    kasih 5 bintang   \n",
       "29    uang ngilang ntah kmna ga recomend sama sekali...   \n",
       "210   bagus cepat mudah dn hemat tentunya trs tingka...   \n",
       "952   gue kasih bintang 1 sengaja gue ikut rules yg ...   \n",
       "583     cepat prosesnya terpercaya lanjutkan lebih baik   \n",
       "\n",
       "                                                 tokens  Sentimen  Pos  Neg  \n",
       "1939                                [kasih, 5, bintang]         1    1    0  \n",
       "29    [uang, ngilang, ntah, kmna, ga, recomend, sama...         0    0    1  \n",
       "210   [bagus, cepat, mudah, dn, hemat, tentunya, trs...         1    1    0  \n",
       "952   [gue, kasih, bintang, 1, sengaja, gue, ikut, r...         0    0    1  \n",
       "583   [cepat, prosesnya, terpercaya, lanjutkan, lebi...         1    1    0  "
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(data_test)\n",
    "\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_encode(text1):\n",
    "    x_1 = tokenizer.texts_to_sequences(text1)\n",
    "    x_1 = pad_sequences(x_1, maxlen= 50, padding='post')\n",
    "    return x_1\n",
    "\n",
    "def get_predict(text1):\n",
    "    #model = loaded_model\n",
    "    result = model.predict(text1)\n",
    "    #result = np.argmax(result1, axis=1)\n",
    "    return result\n",
    "\n",
    "\n",
    "data_kalimat = ['lumayan bagus fiturnya, tapi kalo bisa ditambahin lagi']\n",
    "encode = get_encode(data_kalimat)\n",
    "result = get_predict(encode)\n",
    "#result = model.predict_classes(encode)\n",
    "\n",
    "labels_test = [1, 0]\n",
    "\n",
    "prediction_labels_test=[]\n",
    "for p in result:\n",
    "    prediction_labels_test.append(labels_test[np.argmax(p)])\n",
    "    \n",
    "print(prediction_labels_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1b5f54e4835e6e96295ff4f9b2e3106cd88f1d758fc4bd43ae981ee911efc60"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
